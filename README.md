# Vehicle Insurance MLOps Pipeline

An **end-to-end MLOps project** that builds, trains, evaluates, and deploys a **Vehicle Insurance Prediction system** using modern ML engineering practices.

This project covers the **complete lifecycle**:

* Data ingestion from **MongoDB Atlas**
* Modular ML pipeline (validation, transformation, training, evaluation)
* Model versioning with **AWS S3**
* **FastAPI** application for prediction & training
* **Dockerized deployment**
* **CI/CD using GitHub Actions**
* Deployment on **AWS EC2**

The goal of this repository is to be **easy to understand**, **reproducible**, and **industry-oriented**.

---

## Problem Statement

Predict whether a customer will respond **Yes/No** to a vehicle insurance offer based on demographic and vehicle-related features.

Target column:

* `Response` â†’ (Yes / No)

---

## High-Level Architecture

```
MongoDB Atlas
     â†“
Data Ingestion
     â†“
Data Validation
     â†“
Data Transformation
     â†“
Model Training
     â†“
Model Evaluation
     â†“
Model Pusher (AWS S3)
     â†“
FastAPI App (Prediction + Training)
     â†“
Docker â†’ CI/CD â†’ AWS EC2
```

---

## Project Structure

```
Vehicle-Insurance-Data-Pipeline/
â”‚
â”œâ”€â”€ app.py                     # FastAPI entry point
â”œâ”€â”€ Dockerfile                 # Docker configuration
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ templates/                 # HTML templates
â”‚   â””â”€â”€ vehicledata.html
â”‚
â”œâ”€â”€ static/                    # CSS / static files
â”‚   â””â”€â”€ css/style.css
â”‚
â”œâ”€â”€ notebook/                  # EDA & MongoDB notebooks
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ constants/             # Global constants
â”‚   â”œâ”€â”€ logger/                # Logging configuration
â”‚   â”œâ”€â”€ exception/             # Custom exception handling
â”‚   â”œâ”€â”€ configuration/         # MongoDB & AWS connections
â”‚   â”œâ”€â”€ data_access/           # MongoDB data access layer
â”‚   â”œâ”€â”€ entity/                # Config & artifact entities
â”‚   â”œâ”€â”€ components/            # Pipeline components
â”‚   â”œâ”€â”€ pipeline/              # Training & prediction pipelines
â”‚   â”œâ”€â”€ utils/                 # Utility functions
â”‚   â””â”€â”€ aws_storage/            # S3 interaction logic
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ aws.yaml                # CI/CD pipeline
â”‚
â””â”€â”€ artifact/                   # Pipeline outputs (ignored in git)
```

---

## Step-by-Step Project Workflow

### 1ï¸âƒ£ Project Setup

* Project template generated using `template.py`
* Local packages managed via `setup.py` and `pyproject.toml`
* Virtual environment created using Conda:

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
```

---

### 2ï¸âƒ£ MongoDB Setup (Data Source)

* MongoDB Atlas cluster (M0 â€“ Free Tier)
* Database user created
* Network access enabled: `0.0.0.0/0`
* Data pushed from notebook into MongoDB collection

Environment variable setup:

```bash
export MONGODB_URL="mongodb+srv://<username>:<password>@..."
```

---

### 3ï¸âƒ£ Logging & Exception Handling

* Centralized logging using Python `logging`
* Custom exception class with:

  * File name
  * Line number
  * Error message

Used across **all pipeline components**.

---

### 4ï¸âƒ£ Data Ingestion

* Reads data from MongoDB Atlas
* Converts data into pandas DataFrame
* Stores raw data in **feature store**
* Splits data into `train.csv` and `test.csv`
* Saves outputs as timestamped artifacts

Artifact example:

```
artifact/<timestamp>/data_ingestion/
â”œâ”€â”€ feature_store/data.csv
â””â”€â”€ ingested/train.csv, test.csv
```

---

### 5ï¸âƒ£ Data Validation

* Schema validation using `schema.yaml`
* Checks:

  * Column names
  * Data types
  * Missing values
* Generates validation report

---

### 6ï¸âƒ£ Data Transformation

* Encoding categorical variables
* Scaling numerical features
* Saves:

  * Transformed train/test `.npy` files
  * Preprocessing object (`preprocessing.pkl`)

---

### 7ï¸âƒ£ Model Training

* Algorithm: **RandomForestClassifier**
* Hyperparameters defined in constants
* Model trained on transformed data
* Evaluation metrics:

  * Accuracy
  * F1-score
  * Precision
  * Recall

A combined model object is created:

* Preprocessing + trained model

---

### 8ï¸âƒ£ Model Evaluation & Model Pusher

* New model compared with existing model
* Threshold-based acceptance
* Accepted model pushed to **AWS S3**

AWS Setup:

* IAM User
* S3 Bucket (`my-model-mlopsproj`)

---

### 9ï¸âƒ£ Prediction Pipeline & FastAPI App

* FastAPI provides:

  * `/` â†’ Prediction UI
  * `/train` â†’ Trigger training pipeline

* HTML + CSS UI

* Model loaded once (cached) for fast predictions

---

### ğŸ”Ÿ CI/CD Pipeline (GitHub Actions)

**CI (GitHub-hosted runner):**

* Build Docker image
* Push image to AWS ECR

**CD (Self-hosted runner on EC2):**

* Pull Docker image
* Stop old container
* Run new container

Secrets used:

* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ACCESS_KEY`
* `AWS_DEFAULT_REGION`
* `ECR_REPO`
* `MONGODB_URL`

---

### 1ï¸âƒ£1ï¸âƒ£ Deployment on AWS EC2

* EC2 Ubuntu server
* Docker installed
* App runs inside container
* Port exposed: **5000**

Access application:

```
http://<EC2_PUBLIC_IP>:5000
```

Note: Some networks (like hostel Wi-Fi) may block custom ports.

---

## How to Run Locally

```bash
python app.py
```

Open browser:

```
http://localhost:5000
```

---

##  Key Learnings

* Modular MLOps pipeline design
* Artifact-based ML workflows
* Real-world CI/CD with self-hosted runners
* Debugging deployment & networking issues
* Production-style ML system design

---

## Future Improvements

* Nginx reverse proxy
* HTTPS
* Model monitoring
* Async inference

---

## Author

**Sadab Ali**
MLOps | Data Science | Machine Learning

---

â­ If you find this project helpful, consider starring the repository!
